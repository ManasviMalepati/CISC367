{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-photograph",
   "metadata": {},
   "source": [
    "### PDF Extractor\n",
    "I extracted a pdf of a Blue Chip article about consumer habits during the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "covered-atlas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the           113\n",
       "and            81\n",
       "to             73\n",
       "of             61\n",
       "in             43\n",
       "a              30\n",
       "shopping       24\n",
       "their          23\n",
       "will           19\n",
       "that           18\n",
       "shoppers       18\n",
       "brands         17\n",
       "grocery        17\n",
       "in-store       16\n",
       "experience     15\n",
       "2020           15\n",
       "for            15\n",
       "with           15\n",
       "blue           15\n",
       "rights         14\n",
       "chip.          14\n",
       "All            14\n",
       "©              14\n",
       "as             14\n",
       "reserved.      14\n",
       "be             13\n",
       "new            13\n",
       "retailers      13\n",
       "have           12\n",
       "online         12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "doc = fitz.open('BlueChip.pdf')\n",
    "text = \"\".join(page.get_text(\"text\") for page in doc)\n",
    "words = pd.Series(text.split())\n",
    "words.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-stone",
   "metadata": {},
   "source": [
    "### Reddit Image Transcriber\n",
    "I used the r/aww reddit thread which contains both photos and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "data = requests.get(\"https://www.reddit.com/r/comics/.json\", headers = {'User-agent': 'your bot 0.1'}).json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affected-albania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://i.redd.it/rxudiqlr5yy61.jpg',\n",
       " 'https://i.redd.it/8cpd7of79vy61.jpg',\n",
       " 'https://i.imgur.com/QOVhelr.png',\n",
       " 'https://i.redd.it/kvuiu3vudyy61.png',\n",
       " 'https://i.redd.it/zs0sm5ckczy61.png',\n",
       " 'https://i.redd.it/s06l1vt9twy61.png',\n",
       " 'https://i.redd.it/hvn1c03n5yy61.jpg',\n",
       " 'https://i.redd.it/l6vcz7p81wy61.png',\n",
       " 'https://i.redd.it/lc85u4y8pzy61.png',\n",
       " 'https://i.redd.it/u75r9fqi2vy61.png',\n",
       " 'https://i.redd.it/5wmhdfnkezy61.jpg',\n",
       " 'https://i.redd.it/qlybqkx4gvy61.jpg',\n",
       " 'https://i.redd.it/uw6qfs3bkyy61.jpg',\n",
       " 'https://i.redd.it/0y466fws5zy61.jpg',\n",
       " 'https://i.redd.it/y9009ob8ovy61.png',\n",
       " 'https://i.redd.it/fcs6xar5bvy61.jpg',\n",
       " 'https://i.redd.it/oadn8h49dvy61.jpg',\n",
       " 'https://i.redd.it/amyod5p5zyy61.jpg',\n",
       " 'https://i.redd.it/q2i54ifn6qy61.jpg',\n",
       " 'https://i.redd.it/gdwwbki7dyy61.png',\n",
       " 'https://i.imgur.com/XSMd6ft.jpg',\n",
       " 'https://i.redd.it/k81hb4tklxy61.png',\n",
       " 'https://i.redd.it/s6wxr8xd7yy61.png',\n",
       " 'https://i.redd.it/pfcbiyfuawy61.png']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The title and url for each post\n",
    "title = []\n",
    "url = []\n",
    "for i in data['data']['children']:\n",
    "    title.append(i['data']['title'])\n",
    "    if(i['data']['url'].endswith('.png') or i['data']['url'].endswith('.jpg')):\n",
    "        url.append(i['data']['url'])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "republican-kansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'wait,',\n",
       " 'i',\n",
       " 'graduated',\n",
       " 'three',\n",
       " 'years',\n",
       " 'ago',\n",
       " '—',\n",
       " '—”',\n",
       " 'BL',\n",
       " 'U',\n",
       " '3)',\n",
       " 'Pa',\n",
       " '®',\n",
       " '+',\n",
       " '&',\n",
       " 'oO',\n",
       " '©',\n",
       " '—',\n",
       " 'oO',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'LISTEN',\n",
       " 'UP',\n",
       " 'SON,',\n",
       " 'FIRST',\n",
       " 'YOU',\n",
       " 'INSERT',\n",
       " 'YOUR',\n",
       " 'CARD',\n",
       " 'TO',\n",
       " 'PAY',\n",
       " '|}',\n",
       " 'THEN,',\n",
       " 'SELECT',\n",
       " 'YOUR',\n",
       " 'FUEL.',\n",
       " 'THEN,',\n",
       " 'YOU',\n",
       " 'INHALE',\n",
       " 'DEEPLY',\n",
       " 'AS',\n",
       " 'THE',\n",
       " 'BENZINE',\n",
       " 'TRIGGERS',\n",
       " 'CHEMICALS',\n",
       " 'IN',\n",
       " 'YOUR',\n",
       " 'BRAINS',\n",
       " 'WARD',\n",
       " 'SYSTEM.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.',\n",
       " 'lyou',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'image,',\n",
       " 'itwas',\n",
       " 'probably',\n",
       " 'deleted.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "words = []\n",
    "for u in url:\n",
    "    response = requests.get(u+'.json')\n",
    "    img = Image.open(io.BytesIO(response.content))\n",
    "    text.append(pytesseract.image_to_string(img))\n",
    "for t in text:\n",
    "    words.extend(t.split())\n",
    "words\n",
    "#from textblob import TextBlob\n",
    "#blob = TextBlob(text)\n",
    "#print(blob.noun_phrases)\n",
    "\n",
    "#Sorted with pandas\n",
    "#pd.Series(blob.word_counts).sort_values().tail(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-massage",
   "metadata": {},
   "source": [
    "### Face Finding\n",
    "I used a stock image of a couple people standing and smiling at the camera for my image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "standing-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import urllib\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "\n",
    "img = cv2.imread('stockphoto.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "biblical-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y,w,h) in faces:\n",
    "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-straight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
